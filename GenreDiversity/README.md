# GenreDiversity

曲名のプレイリストレコメンドを強化学習で実現するプロジェクト。DQNとREINFORCEの2つのアルゴリズムを実装。

## プロジェクト構成

```
.
├── spotify_dqn_env.py          # 強化学習環境の定義
├── dqn_agent.py                # DQNエージェントの実装
├── rl_playlist_agent.py        # REINFORCEエージェントの実装
├── generate_sample_songs.py    # サンプル曲データの生成
├── main.py                     # DQN学習の実行スクリプト
├── recommend.py                # 推薦曲の推論スクリプト
├── songs_100.csv               # 生成される曲データ
├── dqn_model.pth              # 学習済みDQNモデル
└── rl_playlist_policy_v2.pth  # 学習済みREINFORCEモデル
```

## クイックスタート

### 1. サンプルデータの生成

```bash
python generate_sample_songs.py
```

100曲のサンプルデータ（`songs_100.csv`）が生成されます。

### 2. モデルの学習

#### DQNで学習する場合

```bash
python main.py
```

#### REINFORCEで学習する場合

```bash
python rl_playlist_agent.py
```

### 3. 推薦曲の取得

```bash
python recommend.py
```

使用するモデルを選択すると、推薦曲トップ10が表示されます。

## 　システム概要

### 強化学習環境 (`spotify_dqn_env.py`)

プレイリスト推薦を強化学習の問題として定式化しています。

- **状態（State）**: ユーザープロファイル（音楽の好みベクトル）
- **行動（Action）**: 曲の選択（100曲から1曲を選ぶ）
- **報酬（Reward）**: 以下の3要素で構成
  - **類似度報酬**: ユーザープロファイルと曲のコサイン類似度
  - **重複ペナルティ**: 同じ曲を繰り返し選ぶと減点
  - **多様性報酬**: ジャンルの多様性に対してボーナス

#### 報酬関数の詳細

```python
# 1. 類似度報酬（基本スコア）
cosine_reward = cosine_similarity(song, user_profile) × 2.0

# 2. 重複ペナルティ
duplicate_penalty = -0.5 × (重複回数 + 1)

# 3. 多様性報酬
# - 前の曲と異なるジャンル: +0.3
# - 全体のジャンル多様性: +0.4 × (ユニークジャンル数 / 全ジャンル数)

total_reward = cosine_reward + duplicate_penalty + diversity_reward
```

### DQNアルゴリズム (`dqn_agent.py`)

Deep Q-Networkを使った価値ベースの学習アルゴリズムです。

**特徴:**
- 3層のニューラルネットワーク（128-128ユニット）
- Experience Replay（リプレイバッファ: 10,000サンプル）
- ε-greedy探索戦略（ε: 1.0 → 0.1に減衰）

**ハイパーパラメータ:**
- エピソード数: 500
- バッチサイズ: 32
- 学習率: 0.001
- 割引率（γ）: 0.99

### REINFORCEアルゴリズム (`rl_playlist_agent.py`)

方策勾配法による直接方策最適化アルゴリズムです。

**特徴:**
- 3層のニューラルネットワーク（128-128ユニット）
- Softmax方策による確率的行動選択
- エントロピー正則化（探索促進）
- 多様性ボーナスの追加実装

**ハイパーパラメータ:**
- エピソード数: 1000
- 学習率: 0.0001
- 割引率（γ）: 0.99
- エントロピー係数: 0.01

## データ形式

### songs_100.csv

| カラム名 | 説明 | 値の範囲 |
|---------|------|---------|
| song_id | 曲ID | 1-100 |
| title | 曲名 | "Song 1" - "Song 100" |
| danceability | 踊りやすさ | 0.3-0.9 |
| energy | エネルギー | 0.3-0.9 |
| tempo | テンポ（BPM） | 110-150 |
| valence | ポジティブさ | 0.2-0.8 |
| genre | ジャンル | Pop, Rock, Hip-Hop, etc. |

### 対応ジャンル

- Pop（ポップ）
- Rock（ロック）
- Hip-Hop（ヒップホップ）
- Electronic（エレクトロニック）
- Classical（クラシック）
- Jazz（ジャズ）
- Country（カントリー）
- Reggae（レゲエ）
- Blues（ブルース）
- Indie（インディー）

##  詳細仕様

### エピソードの終了条件

- 10曲選択した時点でエピソード終了
- 最大でも全曲数（100曲）でも終了

### 状態の更新

現在の実装では、状態（`next_state`）は常にユーザープロファイルで固定されています。将来的には、選曲履歴を反映した動的な状態表現への拡張が可能です。

### モデルの推論

`recommend.py`を実行すると、学習済みモデルを使って推薦曲を取得できます。

- DQN: Q値の上位10曲を推薦
- REINFORCE: 方策の確率が高い上位10曲を推薦

## 学習曲線の可視化

REINFORCEの学習時には、エピソードごとの累積報酬がグラフで表示されます。学習の進行とともに報酬が上昇していることを確認できます。

##  必要なライブラリ

```bash
pip install torch numpy pandas matplotlib
```

##  改善案・今後の展開

1. **状態表現の改善**
   - 選曲履歴を状態に組み込む
   - LSTMやTransformerで時系列パターンを学習

2. **報酬設計の洗練**
   - ユーザーフィードバックの統合
   - スキップ率や再生時間の考慮

3. **実データへの適用**
   - Spotify APIからの実データ取得
   - より多くの特徴量（音響特徴、メタデータ）の活用

4. **オフライン評価指標**
   - NDCG, MAP, MRRなどの情報検索指標
   - 多様性・新規性指標

5. **ハイブリッドアプローチ**
   - 協調フィルタリングとの組み合わせ
   - コンテンツベース推薦との統合

##  ライセンス

このプロジェクトは教育・研究目的で作成されています。

##  コントリビューション

N/A