import numpy as np
import torch
import torch.nn as nn
import pandas as pd
import json
import os
from datetime import datetime

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

LSTM_HIDDEN_SIZE = [256, 128, 64]

class LSTMUserSimulator(nn.Module):
    """è»½é‡ç‰ˆLSTMã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"""
    def __init__(self, input_size, hidden_sizes=LSTM_HIDDEN_SIZE):
        super(LSTMUserSimulator, self).__init__()
        self.hidden_sizes = hidden_sizes
        
        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True)
        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True)
        self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True)
        
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_sizes[2], 32),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x, hidden=None):
        out1, hidden1 = self.lstm1(x, hidden[0] if hidden else None)
        out2, hidden2 = self.lstm2(out1, hidden[1] if hidden else None)
        out3, hidden3 = self.lstm3(out2, hidden[2] if hidden else None)
        
        completion_prob = self.output_layer(out3)
        return completion_prob, (hidden1, hidden2, hidden3)

class DuelingActionHeadDQN(nn.Module):
    """Dueling Architecture + Action Head DQN"""
    def __init__(self, state_dim, action_feature_dim):
        super(DuelingActionHeadDQN, self).__init__()
        
        self.state_net = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        self.action_net = nn.Sequential(
            nn.Linear(action_feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        self.value_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.advantage_stream = nn.Sequential(
            nn.Linear(128 + 64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, state, action_features):
        if state.dim() == 1:
            state = state.unsqueeze(0)
        state_features = self.state_net(state)
        
        if action_features.dim() == 2:
            if action_features.size(0) == state.size(0):
                action_features = action_features.unsqueeze(1)
            else:
                action_features = action_features.unsqueeze(0)

        batch_size = state_features.size(0)
        if action_features.size(0) == 1 and batch_size > 1:
            action_features = action_features.expand(batch_size, -1, -1)
        
        action_features_processed = self.action_net(action_features.view(-1, action_features.size(-1)))
        action_features_processed = action_features_processed.view(batch_size, -1, 64)
        
        state_features_expanded = state_features.unsqueeze(1).expand(-1, action_features.size(1), -1)
        
        value = self.value_stream(state_features)
        num_actions = action_features.size(1)
        value = value.expand(-1, num_actions)
        
        combined = torch.cat([state_features_expanded, action_features_processed], dim=-1)
        advantage = self.advantage_stream(combined.view(-1, combined.size(-1))).view(batch_size, -1)
        
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        
        return q_values

class MusicEnvironment:
    """éŸ³æ¥½æ¨è–¦ç’°å¢ƒï¼ˆæ¨è«–ç”¨ï¼‰"""
    def __init__(self, user_simulator, track_pool_size=1000, session_length=20, state_dim=40):
        self.user_simulator = user_simulator
        self.track_pool_size = track_pool_size
        self.session_length = session_length
        self.state_dim = state_dim
        self.current_step = 0
        self.track_history = []
        self.response_history = []
        
    def reset(self):
        self.current_step = 0
        self.track_history = [0.0] * (self.state_dim // 2)
        self.response_history = [0.0] * (self.state_dim // 2)
        return self._get_state()

    def step(self, action):
        state = self._get_state()
        state_tensor = torch.FloatTensor([state + [float(action)]]).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            response_prob, _ = self.user_simulator(state_tensor)
        response = float(response_prob.squeeze())
        
        self.track_history[self.current_step] = float(action)
        self.response_history[self.current_step] = response
        self.current_step += 1
        
        base_reward = response
        
        if float(action) in self.track_history[:self.current_step-1]:
            base_reward *= 0.2
        
        if self.current_step >= self.session_length:
            responses = np.array(self.response_history)
            avg_response = np.mean(responses)
            
            if avg_response > 0.9:
                base_reward += 2.0
            elif avg_response > 0.8:
                base_reward += 1.0
        
        done = self.current_step >= self.session_length
        return self._get_state(), base_reward, done

    def _get_state(self):
        return self.track_history + self.response_history

def load_track_info(file_path=None):
    """ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã®èª­ã¿è¾¼ã¿ï¼ˆã‚«ãƒ©ãƒ åã®è‡ªå‹•æ¤œå‡ºï¼‰"""
    possible_paths = [
        'data/tracks.csv',
        'tracks.csv',
        'data/track_data.csv',
        '../data/tracks.csv'
    ]
    
    if file_path:
        possible_paths.insert(0, file_path)
    
    for path in possible_paths:
        try:
            print(f" ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã‚’ {path} ã‹ã‚‰èª­ã¿è¾¼ã¿è©¦è¡Œä¸­...")
            tracks_df = pd.read_csv(path)
            
            # ã‚«ãƒ©ãƒ åã‚’ç¢ºèª
            print(f" ã‚«ãƒ©ãƒ å: {list(tracks_df.columns)}")
            
            # track_idã‚«ãƒ©ãƒ ã®æ¤œå‡º
            track_id_col = None
            for col in ['track_id', 'id', 'trackId', 'track_no']:
                if col in tracks_df.columns:
                    track_id_col = col
                    break
            
            if track_id_col is None:
                print(f" âš ï¸ track_id ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã®ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™")
                track_id_col = tracks_df.columns[0]
            
            # titleã‚«ãƒ©ãƒ ã®æ¤œå‡º
            title_col = None
            for col in ['title', 'name', 'track_name', 'song_name', 'song']:
                if col in tracks_df.columns:
                    title_col = col
                    break
            
            if title_col is None:
                print(f" âš ï¸ title ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                # track_idã‚’ãã®ã¾ã¾è¡¨ç¤º
                track_titles = {int(tid): f"Track {tid}" for tid in tracks_df[track_id_col]}
            else:
                # æ­£å¸¸ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã¿
                track_titles = {}
                for _, row in tracks_df.iterrows():
                    tid = int(row[track_id_col])
                    title = row[title_col]
                    # NaNã‚„ç©ºæ–‡å­—åˆ—ã®å‡¦ç†
                    if pd.isna(title) or str(title).strip() == '':
                        track_titles[tid] = f"Track {tid}"
                    else:
                        track_titles[tid] = str(title)
            
            print(f" æˆåŠŸ: {len(track_titles)} ä»¶ã®ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            print(f"   ä½¿ç”¨ã‚«ãƒ©ãƒ : track_id={track_id_col}, title={title_col}")
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            print(f"\n ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
            for i, (tid, title) in enumerate(list(track_titles.items())[:5]):
                print(f"   ID {tid:4d}: {title}")
            print()
            
            return track_titles, tracks_df
            
        except FileNotFoundError:
            continue
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(" âš ï¸ ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    dummy_titles = {i: f"Track {i}" for i in range(1000)}
    return dummy_titles, None

def load_models(model_path, state_dim=40, action_feature_dim=64):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆAction Featureså«ã‚€ï¼‰"""
    print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«ã‚’ {model_path} ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
    
    # DQNãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    policy_net = DuelingActionHeadDQN(state_dim, action_feature_dim).to(DEVICE)
    policy_net.load_state_dict(torch.load(f'{model_path}/policy_net.pth', map_location=DEVICE))
    policy_net.eval()
    print("âœ… DQNãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # Action Featuresã®èª­ã¿è¾¼ã¿
    action_features = None
    try:
        action_features = torch.load(f'{model_path}/action_features.pth', map_location=DEVICE)
        print(f"âœ… Action Features ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (shape: {action_features.shape})")
    except FileNotFoundError:
        print("âš ï¸  Action Features ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã—ã¾ã™ã€‚")
        torch.manual_seed(42)
        action_features = torch.randn(1000, action_feature_dim, device=DEVICE)

    # User Simulatorã®èª­ã¿è¾¼ã¿
    user_simulator = None
    try:
        user_simulator = LSTMUserSimulator(state_dim + 1).to(DEVICE)
        user_simulator.load_state_dict(torch.load(f'{model_path}/user_simulator.pth', map_location=DEVICE))
        user_simulator.eval()
        print("âœ… User Simulatorã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print("âš ï¸  User SimulatorãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿
    try:
        with open(f'{model_path}/metrics.json', 'r') as f:
            metrics = json.load(f)
        print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        print(f"   - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {metrics.get('architecture', 'N/A')}")
        print(f"   - æœ€è‰¯å ±é…¬: {metrics.get('best_avg_reward', 'N/A'):.2f}")
        if 'test_results' in metrics:
            print(f"   - ãƒ†ã‚¹ãƒˆå¿œç­”ç‡: {metrics['test_results'].get('avg_response', 'N/A'):.3f}")
    except FileNotFoundError:
        metrics = None
        print("âš ï¸  ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        metrics = None
        print(f"âš ï¸  ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    print()
    return policy_net, user_simulator, metrics, action_features

def generate_playlist(policy_net, user_simulator, action_features, 
                     track_pool_size=1000, session_length=20, 
                     state_dim=40, temperature=0.3, seed=None):
    """ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®ç”Ÿæˆ"""
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)
    
    env = MusicEnvironment(user_simulator, track_pool_size, session_length, state_dim)
    state = env.reset()
    
    playlist = []
    responses = []
    q_values_history = []
    total_reward = 0
    
    with torch.no_grad():
        for step in range(session_length):
            state_tensor = torch.FloatTensor(state).to(DEVICE)
            
            # Qå€¤ã®è¨ˆç®—
            q_values = policy_net(state_tensor, action_features)
            
            # æ—¢ã«é¸æŠã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚¯ã«ãƒšãƒŠãƒ«ãƒ†ã‚£
            state_array = np.array(state)
            track_history = state_array[:len(state_array)//2]
            penalties = torch.zeros_like(q_values)
            for i in range(action_features.size(0)):
                if i in track_history:
                    penalties[0, i] = -1e10
            
            q_values_adjusted = q_values + penalties
            
            # Temperature scalingã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if temperature > 0:
                q_values_scaled = q_values_adjusted / temperature
                probs = torch.softmax(q_values_scaled, dim=1)
                action = torch.multinomial(probs[0], 1).item()
            else:
                action = torch.argmax(q_values_adjusted).item()
            
            # ç’°å¢ƒã§ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            next_state, reward, done = env.step(action)
            
            # çµæœã®è¨˜éŒ²
            playlist.append(int(action))
            responses.append(float(env.response_history[env.current_step-1]))
            q_values_history.append(float(q_values[0, action].item()))
            total_reward += reward
            
            state = next_state
    
    return {
        'playlist': playlist,
        'responses': responses,
        'q_values': q_values_history,
        'total_reward': total_reward,
        'average_response': np.mean(responses),
        'response_std': np.std(responses),
        'min_response': np.min(responses),
        'max_response': np.max(responses)
    }

def analyze_playlist(result, track_titles=None, tracks_df=None, save_path=None):
    """ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®è©³ç´°åˆ†æ"""
    
    print("\n" + "=" * 80)
    print("ğŸµ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆçµæœã®åˆ†æ")
    print("=" * 80)
    
    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"  ç·åˆå ±é…¬:        {result['total_reward']:6.2f}")
    print(f"  å¹³å‡å¿œç­”ã‚¹ã‚³ã‚¢:  {result['average_response']:.3f}")
    print(f"  å¿œç­”ã®æ¨™æº–åå·®:  {result['response_std']:.3f}")
    print(f"  æœ€å°å¿œç­”ã‚¹ã‚³ã‚¢:  {result['min_response']:.3f}")
    print(f"  æœ€å¤§å¿œç­”ã‚¹ã‚³ã‚¢:  {result['max_response']:.3f}")
    
    # å“è³ªè©•ä¾¡
    high_quality = sum(1 for r in result['responses'] if r > 0.95)
    good_quality = sum(1 for r in result['responses'] if 0.9 < r <= 0.95)
    medium_quality = sum(1 for r in result['responses'] if 0.8 < r <= 0.9)
    
    print(f"\nğŸ¯ å“è³ªåˆ†å¸ƒ:")
    print(f"  é«˜å“è³ª (>0.95):  {high_quality:2d} æ›² ({high_quality/len(result['responses'])*100:.1f}%)")
    print(f"  è‰¯å“è³ª (0.9-0.95): {good_quality:2d} æ›² ({good_quality/len(result['responses'])*100:.1f}%)")
    print(f"  ä¸­å“è³ª (0.8-0.9):  {medium_quality:2d} æ›² ({medium_quality/len(result['responses'])*100:.1f}%)")
    
    # ãƒˆãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ
    print(f"\nğŸ¼ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆè©³ç´°:")
    print("-" * 80)
    
    for i, (track_id, response, q_value) in enumerate(
        zip(result['playlist'], result['responses'], result['q_values']), 1
    ):
        # ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã®å–å¾—
        if track_titles and track_id in track_titles:
            title = track_titles[track_id]
        else:
            title = f"Track {track_id}"
        
        # è¿½åŠ æƒ…å ±
        extra_info = ""
        if tracks_df is not None:
            # track_idã‚«ãƒ©ãƒ ã®æ¤œå‡º
            id_col = None
            for col in ['track_id', 'id', 'trackId']:
                if col in tracks_df.columns:
                    id_col = col
                    break
            
            if id_col:
                track_row = tracks_df[tracks_df[id_col] == track_id]
                if not track_row.empty:
                    if 'artist' in track_row.columns:
                        artist = track_row['artist'].values[0]
                        if pd.notna(artist) and str(artist).strip():
                            extra_info = f" - {artist}"
        
        # å¿œç­”ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è©•ä¾¡ãƒãƒ¼ã‚¯
        if response > 0.95:
            mark = "ğŸŒŸ"
        elif response > 0.9:
            mark = "â­"
        elif response > 0.8:
            mark = "âœ¨"
        else:
            mark = "  "
        
        print(f"{mark} {i:2d}. [ID:{track_id:4d}] {title:50s}{extra_info}")
        print(f"      å¿œç­”: {response:.3f} | Qå€¤: {q_value:7.3f}")
    
    print("-" * 80)
    
    # ä¿å­˜
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump({
                'playlist': result['playlist'],
                'responses': result['responses'],
                'q_values': result['q_values'],
                'statistics': {
                    'total_reward': result['total_reward'],
                    'average_response': result['average_response'],
                    'response_std': result['response_std']
                }
            }, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ä¿å­˜: {save_path}")

def generate_multiple_playlists(policy_net, user_simulator, action_features, 
                                n_playlists=5, **kwargs):
    """è¤‡æ•°ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦æ¯”è¼ƒ"""
    print(f"\nğŸ”„ {n_playlists}å€‹ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...\n")
    
    results = []
    for i in range(n_playlists):
        print(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {i+1}/{n_playlists} ç”Ÿæˆä¸­...")
        result = generate_playlist(
            policy_net, user_simulator, action_features,
            seed=42 + i,
            **kwargs
        )
        results.append(result)
    
    # æ¯”è¼ƒçµ±è¨ˆ
    print("\nğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®æ¯”è¼ƒ:")
    print("-" * 60)
    for i, result in enumerate(results, 1):
        print(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {i}: å¹³å‡å¿œç­” {result['average_response']:.3f} "
              f"(ç¯„å›²: {result['min_response']:.3f}-{result['max_response']:.3f})")
    
    # æœ€è‰¯ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’é¸æŠ
    best_idx = np.argmax([r['average_response'] for r in results])
    print(f"\nğŸ† æœ€è‰¯ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ: #{best_idx + 1}")
    
    return results, best_idx

def main():
    print("\n" + "=" * 80)
    print("ğŸµ éŸ³æ¥½ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    print("=" * 80 + "\n")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    model_path = 'saved_models/20251113_100051'  # æ–°ã—ãå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«
    state_dim = 40
    action_feature_dim = 64
    track_pool_size = 265  # å®Ÿéš›ã®CSVã®ãƒˆãƒ©ãƒƒã‚¯æ•°ã«åˆã‚ã›ã‚‹
    session_length = 20
    temperature = 0.3
    n_playlists = 3
    
    # ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã®èª­ã¿è¾¼ã¿ï¼ˆè‡ªå‹•æ¤œå‡ºï¼‰
    track_titles, tracks_df = load_track_info()
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã¨Action Featuresã®èª­ã¿è¾¼ã¿
        policy_net, user_simulator, metrics, action_features = load_models(
            model_path, state_dim, action_feature_dim
        )
        
        if action_features is None:
            print("âš ï¸  Action FeaturesãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ¨è«–ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return
        
        # è¤‡æ•°ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        results, best_idx = generate_multiple_playlists(
            policy_net,
            user_simulator,
            action_features,
            n_playlists=n_playlists,
            track_pool_size=track_pool_size,
            session_length=session_length,
            state_dim=state_dim,
            temperature=temperature
        )
        
        # æœ€è‰¯ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’è©³ç´°åˆ†æ
        analyze_playlist(
            results[best_idx],
            track_titles,
            tracks_df,
            save_path=f'generated_playlists/playlist_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        
    except FileNotFoundError as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("\nå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - {model_path}/policy_net.pth")
        print(f"  - {model_path}/action_features.pth (å¿…é ˆ)")
        print(f"  - {model_path}/user_simulator.pth (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
        print(f"  - {model_path}/metrics.json (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")

if __name__ == "__main__":
    main()