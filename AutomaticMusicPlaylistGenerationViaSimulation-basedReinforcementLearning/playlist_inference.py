import numpy as np
import torch
import torch.nn as nn
import pandas as pd
import json
import os
from datetime import datetime

# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ===== ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®å®šç¾© =====
# ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜å®šç¾©ãŒå¿…è¦ï¼‰

LSTM_HIDDEN_SIZE = [256, 128, 64]

class LSTMUserSimulator(nn.Module):
    """è»½é‡ç‰ˆLSTMã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"""
    def __init__(self, input_size, hidden_sizes=LSTM_HIDDEN_SIZE):
        super(LSTMUserSimulator, self).__init__()
        self.hidden_sizes = hidden_sizes
        
        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True)
        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True)
        self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True)
        
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_sizes[2], 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x, hidden=None):
        out1, hidden1 = self.lstm1(x, hidden[0] if hidden else None)
        out2, hidden2 = self.lstm2(out1, hidden[1] if hidden else None)
        out3, hidden3 = self.lstm3(out2, hidden[2] if hidden else None)
        
        completion_prob = self.output_layer(out3)
        return completion_prob, (hidden1, hidden2, hidden3)

class DuelingActionHeadDQN(nn.Module):
    """Dueling Architecture + Action Head DQN"""
    def __init__(self, state_dim, action_feature_dim):
        super(DuelingActionHeadDQN, self).__init__()
        
        self.state_net = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        self.action_net = nn.Sequential(
            nn.Linear(action_feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        self.value_stream = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.advantage_stream = nn.Sequential(
            nn.Linear(128 + 64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, state, action_features):
        if state.dim() == 1:
            state = state.unsqueeze(0)
        state_features = self.state_net(state)
        
        if action_features.dim() == 2:
            if action_features.size(0) == state.size(0):
                action_features = action_features.unsqueeze(1)
            else:
                action_features = action_features.unsqueeze(0)

        batch_size = state_features.size(0)
        if action_features.size(0) == 1 and batch_size > 1:
            action_features = action_features.expand(batch_size, -1, -1)
        
        action_features_processed = self.action_net(action_features.view(-1, action_features.size(-1)))
        action_features_processed = action_features_processed.view(batch_size, -1, 64)
        
        state_features_expanded = state_features.unsqueeze(1).expand(-1, action_features.size(1), -1)
        
        value = self.value_stream(state_features)
        num_actions = action_features.size(1)
        value = value.expand(-1, num_actions)
        
        combined = torch.cat([state_features_expanded, action_features_processed], dim=-1)
        advantage = self.advantage_stream(combined.view(-1, combined.size(-1))).view(batch_size, -1)
        
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        
        return q_values

class MusicEnvironment:
    """éŸ³æ¥½æ¨è–¦ç’°å¢ƒï¼ˆæ¨è«–ç”¨ï¼‰"""
    def __init__(self, user_simulator, track_pool_size=1000, session_length=20, state_dim=40):
        self.user_simulator = user_simulator
        self.track_pool_size = track_pool_size
        self.session_length = session_length
        self.state_dim = state_dim
        self.current_step = 0
        self.track_history = []
        self.response_history = []
        
    def reset(self):
        self.current_step = 0
        self.track_history = [0.0] * (self.state_dim // 2)
        self.response_history = [0.0] * (self.state_dim // 2)
        return self._get_state()

    def step(self, action):
        state = self._get_state()
        state_tensor = torch.FloatTensor([state + [float(action)]]).unsqueeze(0).to(DEVICE)
        
        with torch.no_grad():
            response_prob, _ = self.user_simulator(state_tensor)
        response = float(response_prob.squeeze())
        
        self.track_history[self.current_step] = float(action)
        self.response_history[self.current_step] = response
        self.current_step += 1
        
        base_reward = response
        
        if float(action) in self.track_history[:self.current_step-1]:
            base_reward *= 0.2
        
        if self.current_step >= self.session_length:
            responses = np.array(self.response_history)
            avg_response = np.mean(responses)
            
            if avg_response > 0.9:
                base_reward += 2.0
            elif avg_response > 0.8:
                base_reward += 1.0
        
        done = self.current_step >= self.session_length
        return self._get_state(), base_reward, done

    def _get_state(self):
        return self.track_history + self.response_history

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====

def load_track_info(file_path=None):
    """ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã®èª­ã¿è¾¼ã¿"""
    possible_paths = [
        'data/tracks.csv',
        'tracks.csv',
        'data/track_data.csv',
        '../data/tracks.csv'
    ]
    
    if file_path:
        possible_paths.insert(0, file_path)
    
    for path in possible_paths:
        try:
            print(f" ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã‚’ {path} ã‹ã‚‰èª­ã¿è¾¼ã¿è©¦è¡Œä¸­...")
            tracks_df = pd.read_csv(path)
            track_titles = dict(zip(tracks_df['track_id'], tracks_df['title']))
            print(f" æˆåŠŸ: {len(track_titles)} ä»¶ã®ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\n")
            return track_titles, tracks_df
        except FileNotFoundError:
            continue
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print(" è­¦å‘Š: ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒˆãƒ©ãƒƒã‚¯IDã®ã¿è¡¨ç¤ºã—ã¾ã™ã€‚")
    return {}, None

def load_models(model_path, state_dim=40, action_feature_dim=64):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
    print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«ã‚’ {model_path} ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
    
    # DQNãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    policy_net = DuelingActionHeadDQN(state_dim, action_feature_dim).to(DEVICE)
    policy_net.load_state_dict(torch.load(f'{model_path}/policy_net.pth', map_location=DEVICE))
    policy_net.eval()
    print("DQNãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # User Simulatorã®èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    user_simulator = None
    try:
        user_simulator = LSTMUserSimulator(state_dim + 1).to(DEVICE)
        user_simulator.load_state_dict(torch.load(f'{model_path}/user_simulator.pth', map_location=DEVICE))
        user_simulator.eval()
        print("User Simulatorã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print("âš ï¸  User SimulatorãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿
    try:
        with open(f'{model_path}/metrics.json', 'r') as f:
            metrics = json.load(f)
        print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        print(f"   - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {metrics.get('architecture', 'N/A')}")
        print(f"   - æœ€è‰¯å ±é…¬: {metrics.get('best_avg_reward', 'N/A'):.2f}")
        print(f"   - ãƒ†ã‚¹ãƒˆå¿œç­”ç‡: {metrics['test_results'].get('avg_response', 'N/A'):.3f}")
    except FileNotFoundError:
        metrics = None
        print("âš ï¸  ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    print()
    return policy_net, user_simulator, metrics

def generate_playlist(policy_net, user_simulator, track_pool_size=1000, session_length=20, 
                     state_dim=40, action_feature_dim=64, temperature=0.3, seed=None):
    """ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®ç”Ÿæˆ
    
    Args:
        temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ (0.1=æ±ºå®šè«–çš„, 1.0=ãƒ©ãƒ³ãƒ€ãƒ çš„)
        seed: å†ç¾æ€§ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
    """
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)
    
    env = MusicEnvironment(user_simulator, track_pool_size, session_length, state_dim)
    state = env.reset()
    
    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å¾´é‡ã®ç”Ÿæˆï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜ã‚·ãƒ¼ãƒ‰ã§å†ç¾å¯èƒ½ï¼‰
    if seed is not None:
        torch.manual_seed(seed + 1)
    action_features = torch.randn(track_pool_size, action_feature_dim, device=DEVICE)
    
    playlist = []
    responses = []
    q_values_history = []
    total_reward = 0
    
    with torch.no_grad():
        for step in range(session_length):
            state_tensor = torch.FloatTensor(state).to(DEVICE)
            
            # Qå€¤ã®è¨ˆç®—
            q_values = policy_net(state_tensor, action_features)
            
            # æ—¢ã«é¸æŠã•ã‚ŒãŸãƒˆãƒ©ãƒƒã‚¯ã«ãƒšãƒŠãƒ«ãƒ†ã‚£
            state_array = np.array(state)
            track_history = state_array[:len(state_array)//2]
            penalties = torch.zeros_like(q_values)
            for i in range(action_features.size(0)):
                if i in track_history:
                    penalties[0, i] = -1e10  # å¤§ããªè² ã®å€¤ã§é™¤å¤–
            
            q_values_adjusted = q_values + penalties
            
            # Temperature scalingã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if temperature > 0:
                q_values_scaled = q_values_adjusted / temperature
                probs = torch.softmax(q_values_scaled, dim=1)
                action = torch.multinomial(probs[0], 1).item()
            else:
                # temperature=0ã¯å®Œå…¨ã«è²ªæ¬²
                action = torch.argmax(q_values_adjusted).item()
            
            # ç’°å¢ƒã§ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            next_state, reward, done = env.step(action)
            
            # çµæœã®è¨˜éŒ²
            playlist.append(int(action))
            responses.append(float(env.response_history[env.current_step-1]))
            q_values_history.append(float(q_values[0, action].item()))
            total_reward += reward
            
            state = next_state
    
    return {
        'playlist': playlist,
        'responses': responses,
        'q_values': q_values_history,
        'total_reward': total_reward,
        'average_response': np.mean(responses),
        'response_std': np.std(responses),
        'min_response': np.min(responses),
        'max_response': np.max(responses)
    }

def analyze_playlist(result, track_titles=None, tracks_df=None, save_path=None):
    """ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®è©³ç´°åˆ†æ"""
    
    print("\n" + "=" * 80)
    print("ğŸµ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆçµæœã®åˆ†æ")
    print("=" * 80)
    
    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"  ç·åˆå ±é…¬:        {result['total_reward']:6.2f}")
    print(f"  å¹³å‡å¿œç­”ã‚¹ã‚³ã‚¢:  {result['average_response']:.3f}")
    print(f"  å¿œç­”ã®æ¨™æº–åå·®:  {result['response_std']:.3f}")
    print(f"  æœ€å°å¿œç­”ã‚¹ã‚³ã‚¢:  {result['min_response']:.3f}")
    print(f"  æœ€å¤§å¿œç­”ã‚¹ã‚³ã‚¢:  {result['max_response']:.3f}")
    
    # å“è³ªè©•ä¾¡
    high_quality = sum(1 for r in result['responses'] if r > 0.95)
    good_quality = sum(1 for r in result['responses'] if 0.9 < r <= 0.95)
    medium_quality = sum(1 for r in result['responses'] if 0.8 < r <= 0.9)
    
    print(f"\nğŸ¯ å“è³ªåˆ†å¸ƒ:")
    print(f"  é«˜å“è³ª (>0.95):  {high_quality:2d} æ›² ({high_quality/len(result['responses'])*100:.1f}%)")
    print(f"  è‰¯å“è³ª (0.9-0.95): {good_quality:2d} æ›² ({good_quality/len(result['responses'])*100:.1f}%)")
    print(f"  ä¸­å“è³ª (0.8-0.9):  {medium_quality:2d} æ›² ({medium_quality/len(result['responses'])*100:.1f}%)")
    
    # ãƒˆãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ
    print(f"\nğŸ¼ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆè©³ç´°:")
    print("-" * 80)
    
    for i, (track_id, response, q_value) in enumerate(
        zip(result['playlist'], result['responses'], result['q_values']), 1
    ):
        # ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã®å–å¾—
        if track_titles:
            title = track_titles.get(track_id, "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
        else:
            title = "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
        
        # è¿½åŠ æƒ…å ±ï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆãªã©ï¼‰
        extra_info = ""
        if tracks_df is not None:
            track_row = tracks_df[tracks_df['track_id'] == track_id]
            if not track_row.empty:
                if 'artist' in track_row.columns:
                    artist = track_row['artist'].values[0]
                    extra_info = f" - {artist}"
        
        # å¿œç­”ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è©•ä¾¡ãƒãƒ¼ã‚¯
        if response > 0.95:
            mark = "ğŸŒŸ"
        elif response > 0.9:
            mark = "â­"
        elif response > 0.8:
            mark = "âœ¨"
        else:
            mark = "  "
        
        print(f"{mark} {i:2d}. [ID:{track_id:4d}] {title:50s}{extra_info}")
        print(f"      å¿œç­”: {response:.3f} | Qå€¤: {q_value:7.3f}")
    
    print("-" * 80)
    
    # ä¿å­˜
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump({
                'playlist': result['playlist'],
                'responses': result['responses'],
                'statistics': {
                    'total_reward': result['total_reward'],
                    'average_response': result['average_response'],
                    'response_std': result['response_std']
                }
            }, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ä¿å­˜: {save_path}")

def generate_multiple_playlists(policy_net, user_simulator, n_playlists=5, **kwargs):
    """è¤‡æ•°ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦æ¯”è¼ƒ"""
    print(f"\nğŸ”„ {n_playlists}å€‹ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆä¸­...\n")
    
    results = []
    for i in range(n_playlists):
        print(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {i+1}/{n_playlists} ç”Ÿæˆä¸­...")
        result = generate_playlist(
            policy_net, user_simulator, 
            seed=42 + i,  # ç•°ãªã‚‹ã‚·ãƒ¼ãƒ‰ã§å¤šæ§˜æ€§ã‚’ç¢ºä¿
            **kwargs
        )
        results.append(result)
    
    # æ¯”è¼ƒçµ±è¨ˆ
    print("\nğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®æ¯”è¼ƒ:")
    print("-" * 60)
    for i, result in enumerate(results, 1):
        print(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {i}: å¹³å‡å¿œç­” {result['average_response']:.3f} "
              f"(ç¯„å›²: {result['min_response']:.3f}-{result['max_response']:.3f})")
    
    # æœ€è‰¯ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’é¸æŠ
    best_idx = np.argmax([r['average_response'] for r in results])
    print(f"\nğŸ† æœ€è‰¯ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ: #{best_idx + 1}")
    
    return results, best_idx

def main():
    print("\n" + "=" * 80)
    print("ğŸµ éŸ³æ¥½ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    print("=" * 80 + "\n")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    model_path = 'saved_models/20251111_102904'  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
    state_dim = 40
    action_feature_dim = 64
    track_pool_size = 1000
    session_length = 20
    temperature = 0.3  # 0.1=æ±ºå®šè«–çš„, 1.0=å¤šæ§˜æ€§é‡è¦–
    n_playlists = 3    # ç”Ÿæˆã™ã‚‹ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆæ•°
    
    # ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±ã®èª­ã¿è¾¼ã¿
    track_titles, tracks_df = load_track_info()
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        policy_net, user_simulator, metrics = load_models(
            model_path, state_dim, action_feature_dim
        )
        
        # è¤‡æ•°ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        results, best_idx = generate_multiple_playlists(
            policy_net,
            user_simulator,
            n_playlists=n_playlists,
            track_pool_size=track_pool_size,
            session_length=session_length,
            state_dim=state_dim,
            action_feature_dim=action_feature_dim,
            temperature=temperature
        )
        
        # æœ€è‰¯ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’è©³ç´°åˆ†æ
        analyze_playlist(
            results[best_idx],
            track_titles,
            tracks_df,
            save_path=f'generated_playlists/playlist_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        
    except FileNotFoundError as e:
        print(f"\n ã‚¨ãƒ©ãƒ¼: {e}")
        print("\nå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - {model_path}/policy_net.pth")
        print(f"  - {model_path}/user_simulator.pth (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
        print(f"  - {model_path}/metrics.json (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")

if __name__ == "__main__":
    main()