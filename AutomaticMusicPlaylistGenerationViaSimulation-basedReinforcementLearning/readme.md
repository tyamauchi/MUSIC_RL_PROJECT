# 自動音楽プレイリスト生成：シミュレーションベース強化学習アプローチ

## 論文情報

- **著者**: Federico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, Matteo Rinaldi, Zhenwen Dai (Spotify)
- **会議**: KDD '23, August 6–10, 2023, Long Beach, CA, USA
- **DOI**: https://doi.org/10.1145/3580305.3599777
## 概要

音楽ストリーミングサービスにおけるプレイリストのパーソナライゼーションに対して、シミュレーションベースの強化学習(RL)フレームワークを提案。従来の協調フィルタリング手法が抱える、オフライン評価指標とオンラインユーザー満足度指標の不一致問題を解決する。

## 主な課題

### 1. 従来手法の限界

- 協調フィルタリングは明示的なコンテンツ品質の仮定に依存
- オフライン目的関数とオンライン満足度指標の不一致
- 音響的一貫性やコンテキスト、最適なアイテム順序を考慮できない
### 2. 行動空間の組み合わせ爆発

- 1000曲のプールから30曲のプレイリストを生成する場合、約10^89通りの可能性
- 既存のRL手法(スレート推薦のslate-MDPやslateQ)は適用不可
- これらの手法はユーザーがスレートから1つまたは0個のアイテムを選択する前提
- プレイリストではユーザーが複数のアイテムを消費するため、この前提が成立しない
## 提案手法

### 1. シミュレーション環境の設計

- ユーザーモデルを構築し、シミュレーション環境の一部として使用
- ユーザーの聴取セッションデータから学習
- エージェントが生成したプレイリストに対するユーザー反応を推定

### 2. 行動空間の分解

- プレイリスト全体を一度に選択するのではなく、逐次的に1曲ずつ選択
- 各ステップでユーザーモデルが反応を予測
- 行動空間を候補プールのサイズに削減し、学習を実行可能に
### 3. ユーザー行動モデル

#### 非逐次モデル(CWM: Contextual World Model)

- トラック特徴とユーザー特徴のみを使用
- 高速で学習が容易
- 最適方策が既知(貪欲ランキング)

#### 逐次モデル(SWM: Sequential World Model)

- LSTMを使用して順序情報を捉える
- 3層構造(500, 200, 200 LSTMユニット)
- より高精度だが学習が困難
### 4. Action Head DQN (AH-DQN)

#### 特徴

- 従来のDQNと異なり、状態とアクションの特徴量の両方を入力
- 各アクションに対してQ値を出力
- 訓練時に見ていないアイテムにも対応可能
- 単一エージェントで多様な候補プールに対応

#### 利点

- 動的に変化する候補プールに対応
- 数億曲のカタログ全体で単一のDQNを訓練可能
- 訓練中に見ていないアイテムにも汎化
## 実験と評価

### 1. 公開データセットでの評価

- **データセット**: Spotifyの公開データセット(1.6億の聴取セッション)
- **設定**: セッション長20、最初の5曲を観測、残り15曲を推薦
- **結果**:

| 方策 | 平均リターン | 標準偏差 |
|------|------------|----------|
| Action Head Policy | 1.94 | 1.27 |
| CWM-GMPC | 2.46 | 1.10 |
| Random | 0.98 | 0.76 |
### 2. オンラインA/Bテスト

- **規模**: 200万ユーザー、280万ユニーク楽曲、400万セッション
- **期間**: 1週間
- **比較対象**:
  - Random: ランダムシャッフル
  - Cosine Similarity: ユーザーとトラックの埋め込みベース
  - CWM-GMPC: ユーザーモデルの貪欲ランキング
  - Action Head Policy: 提案手法

#### 主要な発見

- CWM-GMPCとAction Head Policyは統計的に区別不可能
- 両方とも制御群と統計的に区別不可能
- オフライン性能とオンライン性能の間に強い相関

#### メトリクス(Agent vs Control)

- 完了カウント: +10.17% (p=0.59)
- スキップ率: -7.8% (p=0.33)
- 完了率: +5.39% (p=0.60)
- セッション長: +8.87% (p=0.53)
### 3. オフライン-オンライン相関の検証

- シミュレーション環境での性能評価が実際のオンライン結果と強く相関
- SWMを用いた独立した評価モデルで方策性能を推定
- バイモーダル分布に対応するため、ガウス混合モデルで報酬分布を分離
## 主な貢献

1. **新しいRLアプローチ**: ユーザーシミュレーションに基づく音楽プレイリスト生成
2. **組み合わせ行動空間の分解**: 逐次的な実行可能なアクションに分解
3. **ユーザー行動モデル**: RNNベースのモデルで逐次的ダイナミクスを学習
4. **改良型DQNエージェント**: シミュレーション環境で訓練
5. **評価手法の確立**: オフライン・オンライン評価の一致性を実証
## 技術的詳細

### 報酬関数

- 各トラックの完了確率の合計
- ユーザーモデルが推定した完了確率を使用
- R(a_t) ∈ [0, 1]

### 状態表現

- ユーザー特徴(興味、過去のインタラクション)
- コンテキスト特徴(時間帯、デバイスタイプなど)
- アイテム特徴(音響特性、人気度など)
- セッション履歴(既に推薦されたトラック、ユーザー応答)

### 訓練手順

1. ランダム方策でデータ収集
2. ユーザーモデルを教師あり学習で訓練
3. シミュレーション環境でDQNエージェントを訓練
4. オフライン評価
5. オンラインA/Bテストで検証
## 実用的な利点

1. **リスク軽減**: オンライン展開前にシミュレーションで評価
2. **スケーラビリティ**: 単一エージェントで多様なユースケースに対応
3. **汎化性**: 未知のアイテムや候補プールにも対応
4. **非近視眼的**: 長期的なユーザー満足度を最適化
## 将来の研究方向

### ユーザーモデルの改善

- より良いユーザー表現の設計
- Transformerなどの異なるアーキテクチャの探索
- ドロップアウトやアンサンブル手法による予測のロバスト性向上

### より複雑なシナリオへの対応

- SWMに対して訓練されたエージェント(AH-SWM)の性能向上
- オンライン学習との組み合わせ
## 結論

シミュレーションベースのRLフレームワークにより、ユーザー満足度を直接最適化する音楽プレイリスト生成を実現。オフライン訓練のみでオンラインでも良好な性能を発揮し、従来のベースラインと同等以上の結果を達成。シミュレーション環境での性能評価が実際のオンライン結果と強く相関することを実証し、実用的なRL適用の可能性を示した。