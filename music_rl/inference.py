import torch
import numpy as np
import json
import os
from datetime import datetime
from config.config import DEVICE, STATE_DIM, ACTION_FEATURE_DIM, SESSION_LENGTH, TRACK_POOL_SIZE
from models.dqn import DuelingActionHeadDQN
from simulators.user_simulator import DeterministicUserSimulator
from agents.dqn_agent import DoubleDQNAgent

# æ›²ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
TRACK_DB = None
try:
    from track_metadata import load_track_metadata, get_track_info
    TRACK_DB = load_track_metadata()
    print(f"âœ“ æ›²ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(TRACK_DB)}æ›²")
except ImportError as e:
    print(f"âš  track_metadata.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("  Track IDã®ã¿è¡¨ç¤ºã—ã¾ã™ã€‚")
except Exception as e:
    print(f"âš  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    print("  Track IDã®ã¿è¡¨ç¤ºã—ã¾ã™ã€‚")


class PlaylistRecommender:
    def __init__(self, model_dir='saved_models'):
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¨è–¦ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
        
        Args:
            model_dir: ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.model_dir = self._find_latest_model(model_dir)
        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {self.model_dir}")
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã¨èª­ã¿è¾¼ã¿
        self.agent = DoubleDQNAgent(STATE_DIM, ACTION_FEATURE_DIM, use_dueling=True)
        model_path = os.path.join(self.model_dir, 'policy_net.pth')
        self.agent.load_model(model_path)
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å¾´é‡ã®èª­ã¿è¾¼ã¿
        action_features_path = os.path.join(self.model_dir, 'action_features.pth')
        self.action_features = torch.load(action_features_path, map_location=DEVICE)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿
        metrics_path = os.path.join(self.model_dir, 'metrics.json')
        if os.path.exists(metrics_path):
            with open(metrics_path, 'r') as f:
                self.metrics = json.load(f)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤ºï¼ˆã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
            if 'avg_reward' in self.metrics:
                print(f"âœ“ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½: å¹³å‡å ±é…¬ {self.metrics['avg_reward']:.2f}")
            else:
                print(f"âœ“ ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {self.metrics}")
        else:
            print("âš  metrics.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.metrics = {}
        
        print(f"âœ“ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°: {self.action_features.size(0)} tracks")
    
    def _find_latest_model(self, base_dir):
        """æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¦‹ã¤ã‘ã‚‹"""
        if not os.path.exists(base_dir):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_dir}")
        
        model_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]
        if not model_dirs:
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {base_dir}")
        
        # æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠ
        latest_dir = sorted(model_dirs)[-1]
        return os.path.join(base_dir, latest_dir)
    
    def generate_playlist(self, session_length=SESSION_LENGTH, avoid_duplicates=True, diversity_bonus=False):
        """
        ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            session_length: ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®é•·ã•
            avoid_duplicates: é‡è¤‡ã‚’é¿ã‘ã‚‹ã‹ã©ã†ã‹
            diversity_bonus: å¤šæ§˜æ€§ãƒœãƒ¼ãƒŠã‚¹ã‚’é©ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        
        Returns:
            playlist: æ¨è–¦ã•ã‚ŒãŸæ›²ã®IDãƒªã‚¹ãƒˆ
            q_values_history: å„ã‚¹ãƒ†ãƒƒãƒ—ã®Qå€¤
        """
        state = [0.0] * STATE_DIM
        playlist = []
        q_values_history = []
        
        for step in range(session_length):
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(DEVICE)
                q_values = self.agent.policy_net(state_tensor, self.action_features)
                
                # é‡è¤‡ãƒšãƒŠãƒ«ãƒ†ã‚£
                if avoid_duplicates:
                    penalties = torch.zeros_like(q_values)
                    track_history = state[:STATE_DIM//2]
                    for i in range(self.action_features.size(0)):
                        if i in track_history:
                            penalties[0, i] = -10.0  # å¼·ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
                    q_values = q_values + penalties
                
                # å¤šæ§˜æ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if diversity_bonus and len(playlist) > 0:
                    # æœ€è¿‘é¸ã‚“ã æ›²ã¨ç•°ãªã‚‹ç‰¹å¾´ã‚’æŒã¤æ›²ã‚’å„ªé‡
                    recent_features = self.action_features[playlist[-1]].unsqueeze(0)
                    similarities = torch.nn.functional.cosine_similarity(
                        self.action_features, recent_features, dim=1
                    )
                    diversity_bonus_values = (1 - similarities) * 0.5
                    q_values[0] += diversity_bonus_values
                
                # æœ€è‰¯ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
                action = torch.argmax(q_values).item()
                q_values_history.append(q_values[0].cpu().numpy())
            
            # çŠ¶æ…‹ã‚’æ›´æ–°ï¼ˆå®Ÿéš›ã®å¿œç­”ã¯ä¸æ˜ãªã®ã§0.8ã¨ä»®å®šï¼‰
            playlist.append(action)
            state[step] = float(action)
            state[STATE_DIM//2 + step] = 0.8  # ä»®ã®å¿œç­”å€¤
        
        return playlist, q_values_history
    
    def generate_multiple_playlists(self, n_playlists=5, **kwargs):
        """
        è¤‡æ•°ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            n_playlists: ç”Ÿæˆã™ã‚‹ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆæ•°
            **kwargs: generate_playlistã«æ¸¡ã™å¼•æ•°
        
        Returns:
            playlists: ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        playlists = []
        for i in range(n_playlists):
            playlist, _ = self.generate_playlist(**kwargs)
            playlists.append(playlist)
        return playlists
    
    def display_playlist(self, playlist, title="æ¨è–¦ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ", show_details=True):
        """
        ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º
        
        Args:
            playlist: æ›²IDã®ãƒªã‚¹ãƒˆ
            title: ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«
            show_details: è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
        """
        print("\n" + "=" * 70)
        print(f"{title}")
        print("=" * 70)
        
        if TRACK_DB and show_details:
            # è©³ç´°è¡¨ç¤º
            for i, track_id in enumerate(playlist, 1):
                info = get_track_info(track_id, TRACK_DB)
                print(f"  {i:2d}. {info['name']:<30} - {info['artist']:<25}")
                print(f"      [{info['genre']:<12}] {info['duration_str']:<6} (äººæ°—åº¦: {info['popularity']})")
        else:
            # ã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤º
            for i, track_id in enumerate(playlist, 1):
                print(f"  {i:2d}. Track #{track_id:3d}")
        
        print("=" * 70 + "\n")
    
    def evaluate_with_simulator(self, playlist, deterministic_sim=None):
        """
        æ±ºå®šè«–çš„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’è©•ä¾¡
        
        Args:
            playlist: è©•ä¾¡ã™ã‚‹æ›²IDã®ãƒªã‚¹ãƒˆ
            deterministic_sim: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
        
        Returns:
            evaluation: è©•ä¾¡çµæœã®è¾æ›¸
        """
        if deterministic_sim is None:
            deterministic_sim = DeterministicUserSimulator(TRACK_POOL_SIZE)
        
        deterministic_sim.reset_user_state()
        
        state = [0.0] * STATE_DIM
        responses = []
        rewards = []
        
        for step, track_id in enumerate(playlist):
            response = deterministic_sim.get_response(state, track_id, step)
            responses.append(response)
            
            # å ±é…¬è¨ˆç®—ï¼ˆç’°å¢ƒã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            base_reward = response
            if track_id in playlist[:step]:
                base_reward *= 0.2
            
            rewards.append(base_reward)
            
            # çŠ¶æ…‹æ›´æ–°
            state[step] = float(track_id)
            state[STATE_DIM//2 + step] = response
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ãƒœãƒ¼ãƒŠã‚¹
        avg_response = np.mean(responses)
        if avg_response > 0.9:
            rewards[-1] += 2.0
        elif avg_response > 0.8:
            rewards[-1] += 1.0
        
        evaluation = {
            'total_reward': sum(rewards),
            'avg_response': avg_response,
            'min_response': min(responses),
            'max_response': max(responses),
            'unique_tracks': len(set(playlist)),
            'duplicate_rate': 1.0 - len(set(playlist)) / len(playlist)
        }
        
        return evaluation, responses
    
    def save_playlist(self, playlist, filename=None, output_dir='playlists'):
        """
        ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            playlist: æ›²IDã®ãƒªã‚¹ãƒˆ
            filename: ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            output_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        os.makedirs(output_dir, exist_ok=True)
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'playlist_{timestamp}.json'
        
        filepath = os.path.join(output_dir, filename)
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'model_dir': self.model_dir,
            'playlist': playlist,
            'length': len(playlist),
            'unique_tracks': len(set(playlist))
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"âœ“ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆä¿å­˜: {filepath}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("\n" + "=" * 60)
    print("ğŸµ éŸ³æ¥½æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆ")
    print("=" * 60 + "\n")
    
    # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    recommender = PlaylistRecommender()
    
    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆè©•ä¾¡ç”¨ï¼‰
    simulator = DeterministicUserSimulator(TRACK_POOL_SIZE)
    
    # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆ
    print("\nã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³1ã€‘æ¨™æº–ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡ãªã—ï¼‰")
    playlist1, _ = recommender.generate_playlist(avoid_duplicates=True, diversity_bonus=False)
    recommender.display_playlist(playlist1, title="æ¨™æº–ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ")
    
    eval1, responses1 = recommender.evaluate_with_simulator(playlist1, simulator)
    print(f"è©•ä¾¡:")
    print(f"  ç·å ±é…¬: {eval1['total_reward']:.2f}")
    print(f"  å¹³å‡å¿œç­”ç‡: {eval1['avg_response']:.3f}")
    print(f"  é‡è¤‡ç‡: {eval1['duplicate_rate']:.1%}")
    
    # å¤šæ§˜æ€§é‡è¦–ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ
    print("\nã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³2ã€‘å¤šæ§˜æ€§é‡è¦–ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ")
    playlist2, _ = recommender.generate_playlist(avoid_duplicates=True, diversity_bonus=True)
    recommender.display_playlist(playlist2, title="å¤šæ§˜æ€§é‡è¦–ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ")
    
    eval2, responses2 = recommender.evaluate_with_simulator(playlist2, simulator)
    print(f"è©•ä¾¡:")
    print(f"  ç·å ±é…¬: {eval2['total_reward']:.2f}")
    print(f"  å¹³å‡å¿œç­”ç‡: {eval2['avg_response']:.3f}")
    print(f"  é‡è¤‡ç‡: {eval2['duplicate_rate']:.1%}")
    
    # è¤‡æ•°ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆ
    print("\nã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³3ã€‘è¤‡æ•°ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆç”Ÿæˆ")
    playlists = recommender.generate_multiple_playlists(n_playlists=3)
    for i, playlist in enumerate(playlists, 1):
        recommender.display_playlist(playlist, title=f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ #{i}")
    
    # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ä¿å­˜
    print("\nã€ä¿å­˜ã€‘")
    recommender.save_playlist(playlist1, filename='standard_playlist.json')
    recommender.save_playlist(playlist2, filename='diverse_playlist.json')
    
    print("\n" + "=" * 60)
    print("âœ“ å®Œäº†")
    print("=" * 60 + "\n")


if __name__ == '__main__':
    main()